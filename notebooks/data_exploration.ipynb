{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and preprocessing\n",
    "\n",
    "- In this notebook, we load in the MRI scans and their segmentations, build a Dataset object for the train and test set.\n",
    "- Then we check some basic stats of the datasets and visualise a few scans.\n",
    "- Finally, we carry out our preprocessing steps and save the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import logging\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from data_utils import Dataset\n",
    "\n",
    "\n",
    "# get TF logger - set it to info for more tracking process\n",
    "log = logging.getLogger('pytorch')\n",
    "log.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test datasets\n",
    "\n",
    "Find all the raw data files and build train and test Dataset objects from the patients' scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file paths to DICOM MRI scans and segmentation images\n",
    "# note that leaderboard samples can be used for training\n",
    "train_scan_files = glob('../data/raw/train/**/*.dcm', recursive=True)\n",
    "train_scan_files += glob('../data/raw/leaderboard/**/*.dcm', recursive=True)\n",
    "test_scan_files = glob('../data/raw/test/**/*.dcm', recursive=True)\n",
    "\n",
    "# ProstateDx-01-0006_corrected_label.nrrd was renamed to ProstateDx-01-0006.nrrd\n",
    "# In the leaderboard and test folders the _truth postfix have been removed from all nrrd files\n",
    "train_seg_files = glob('../data/raw/train/**/*.nrrd', recursive=True)\n",
    "train_seg_files += glob('../data/raw/leaderboard/**/*.nrrd', recursive=True)\n",
    "test_seg_files = glob('../data/raw/test/**/*.nrrd', recursive=True)\n",
    "\n",
    "# build datasets from file paths\n",
    "train_dataset = Dataset(scan_files=train_scan_files, seg_files=train_seg_files)\n",
    "test_dataset = Dataset(scan_files=test_scan_files, seg_files=test_seg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check basic stats\n",
    "Check number of patients in train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = len(train_dataset.patient_ids)\n",
    "test_n = len(test_dataset.patient_ids)\n",
    "train_scan_nums = [p.scans.shape[0] for p in train_dataset.patients.values()]\n",
    "test_scan_nums = [p.scans.shape[0] for p in test_dataset.patients.values()]\n",
    "\n",
    "print('Number of patients in train dataset: %d' % train_n)\n",
    "print('Number of patients in test dataset: %d' % test_n)\n",
    "print('Number of scans in train dataset: %d' % sum(train_scan_nums))\n",
    "print('Number of scans in test dataset: %d' % sum(test_scan_nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of number of scans in train and test datasets.\n",
    "- They both seem bi-modal with roughly the same peaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].set_title('# scans in train dataset')\n",
    "ax[0].hist(train_scan_nums, bins=10)\n",
    "ax[1].set_title('# scans in test dataset')\n",
    "ax[1].hist(test_scan_nums, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that none of the patients have scans from mixed manufacturers and with mixed slice thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract manufacturer and thickness sets from each patient\n",
    "train_manufacturers = [p.manufacturers for p in train_dataset.patients.values()]\n",
    "train_thicknesses = [p.thicknesses for p in train_dataset.patients.values()]\n",
    "test_manufacturers = [p.manufacturers for p in test_dataset.patients.values()]\n",
    "test_thicknesses = [p.thicknesses for p in test_dataset.patients.values()]\n",
    "\n",
    "# check if any patient has slices from two different manufacturers or thicknesses - NO\n",
    "for m in train_manufacturers + test_manufacturers:\n",
    "    assert len(m) == 1\n",
    "\n",
    "for t in train_thicknesses + test_thicknesses:\n",
    "    assert len(t) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create summary table \n",
    "\n",
    "Collate all information into a pandas DataFrame from the datasets so we can analyse it easily later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse all list of sets to simple list\n",
    "train_manufacturers = [list(i)[0] for i in train_manufacturers]\n",
    "train_thicknesses = [list(i)[0] for i in train_thicknesses]\n",
    "test_manufacturers = [list(i)[0] for i in test_manufacturers]\n",
    "test_thicknesses = [list(i)[0] for i in test_thicknesses]\n",
    "\n",
    "# extract scan width, height and max value\n",
    "train_widths = [p.scans.shape[1] for p in train_dataset.patients.values()]\n",
    "train_heights = [p.scans.shape[2] for p in train_dataset.patients.values()]\n",
    "train_max = [p.scans.max() for p in train_dataset.patients.values()]\n",
    "test_widths = [p.scans.shape[1] for p in test_dataset.patients.values()]\n",
    "test_heights = [p.scans.shape[2] for p in test_dataset.patients.values()]\n",
    "test_max = [p.scans.max() for p in test_dataset.patients.values()]\n",
    "\n",
    "# calculate contingency table from them\n",
    "df_summary = pd.DataFrame(\n",
    "    list(\n",
    "        zip(\n",
    "            train_dataset.patient_ids + test_dataset.patient_ids,\n",
    "            train_manufacturers + test_manufacturers,\n",
    "            train_thicknesses + test_thicknesses,\n",
    "            train_widths + test_widths,\n",
    "            train_heights + test_heights,\n",
    "            train_max + test_max,\n",
    "            train_scan_nums + test_scan_nums,\n",
    "            ['train'] * train_n + ['test'] * test_n\n",
    "        )\n",
    "    ), \n",
    "    columns = ['patient_id', 'manufacturer', 'thickness', \n",
    "               'width', 'heigth', 'max_val', 'scan_num', 'dataset']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the test and train datasets have been properly stratified with respect to manufacturer, i.e. half of the sample are from Siemens and half of them are from Philips.\n",
    "- However, the test dataset doesn't have slices of 4mm thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.drop(\n",
    "    ['width', 'heigth', 'scan_num', 'max_val'], axis=1\n",
    ").groupby(\n",
    "    ['dataset', 'manufacturer', 'thickness']\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Philips is the higher resolution machine, all scans are rectangular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.drop(\n",
    "    ['thickness', 'max_val', 'scan_num'], axis=1\n",
    ").groupby(\n",
    "    ['dataset', 'manufacturer', 'width', 'heigth']\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There's a large variation in the scans' maximum values and there are some clear outliers too (Siemens scan with max=65283, coming from the 18th scan of patient Prostate3T-01-0018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.drop(\n",
    "    ['thickness', 'patient_id', 'scan_num'], axis=1\n",
    ").groupby(\n",
    "    ['dataset', 'manufacturer', 'width', 'heigth']\n",
    ").agg(['min', 'max', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's the reason for those bi-modal histograms above, Philips mave higher number of scans on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.drop(\n",
    "    ['thickness', 'max_val', 'width', 'heigth', 'patient_id'], axis=1\n",
    ").groupby(\n",
    "    ['dataset', 'manufacturer']\n",
    ").agg(['min', 'max', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise scans\n",
    "\n",
    "Each patient's scans can be viewed as an animation or as a tiled figure. Let's have a look at some of these.\n",
    "\n",
    "**Note** you'll need to re-execute the cell to watch the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animation\n",
    "patient_id = train_dataset.patient_ids[23]\n",
    "train_dataset.patients[patient_id].patient_anim_scans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiled figure\n",
    "patient_id = train_dataset.patient_ids[2]\n",
    "train_dataset.patients[patient_id].patient_tile_scans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate class frequency\n",
    "\n",
    "The 3 classes are imbalanced, calculate their frequency in the data. This will inform our weighting scheme that we use with the loss function at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq = np.zeros(3)\n",
    "for i in range(len(train_dataset.patients.keys())):\n",
    "    patient_id = train_dataset.patient_ids[i]\n",
    "    seg = train_dataset.patients[patient_id].seg\n",
    "    class0 = np.count_nonzero(seg == 0)\n",
    "    class1 = np.count_nonzero(seg == 1)\n",
    "    class2 = np.count_nonzero(seg == 2)\n",
    "    class_freq += np.array([class0, class1, class2])\n",
    "class_freq = class_freq / class_freq.sum()\n",
    "inv_class_freq = 1/class_freq\n",
    "norm_inv_class_freq = inv_class_freq / inv_class_freq.sum()\n",
    "norm_inv_class_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "As we've seen from the summary stats above the scans are non-normalised and span a wide range of maximal values, resolution and number of scans. \n",
    "\n",
    "The `preprocess_dataset` method:\n",
    "  - normalises the scans to be between zero and one\n",
    "  - resizes (and downsample) each scan and target segmentation image to the same width and height (128, 128)\n",
    "    - since 3D U-Net is fully convolutional, this isn't needed necessarily but it reduces the required memory at training time\n",
    "  - cap depth of scans, i.e. the number of scans across the patients: <32\n",
    "    - this ensures that with a 4 layer deep 3D U-Net we'll get matching dimensions when we concatenate the shurtcuts in the network \n",
    "    - we discard extra scans (i.e. more 32) and patients with less will be padded with zeros by TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, save preprocessed but non-resized test dataset\n",
    "\n",
    "The performance evaluation has to be done ono original (i.e. non rescaled images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_non_resized = copy.deepcopy(test_dataset)\n",
    "test_dataset_non_resized.preprocess_dataset(resize=False, width=_, height=_, max_scans=32)\n",
    "test_dataset_non_resized.save_dataset('../data/processed/test_dataset.pckl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's resize and preprocess both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.preprocess_dataset(resize= True,width=128, height=128, max_scans=32)\n",
    "test_dataset.preprocess_dataset(resize= True,width=128, height=128, max_scans=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the scans and targets still look reasonable on the previous tiled example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the target is now a one-hot tensor, so we only show the 2nd class\n",
    "patient_id = train_dataset.patient_ids[2]\n",
    "train_dataset.patients[patient_id].patient_tile_scans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check that the preprocessing worked for all images. Specifically we should find that:\n",
    "- the number of scans are 32 or less for all patients\n",
    "- the resolution is 128 by 128\n",
    "- the maximum value of the scans is less than or equal to 1\n",
    "- the corresponding target tensor has an extra dimension, corresponding to the one hot encoding of the 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_dataset, test_dataset]\n",
    "for dataset in datasets:\n",
    "    for i in range(len(dataset.patients.keys())):\n",
    "        patient_id = dataset.patient_ids[i]\n",
    "        scans = dataset.patients[patient_id].scans \n",
    "        seg = dataset.patients[patient_id].seg\n",
    "\n",
    "        assert(scans.shape[1:] == (128, 128))\n",
    "        assert(scans.shape[0] <= 32)\n",
    "        assert(scans.max() <= 1)\n",
    "\n",
    "        assert(seg.shape[1:3] == (128, 128))\n",
    "        assert(seg.shape[0] <= 32)\n",
    "        assert(seg.shape[3] == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resized datasets\n",
    "\n",
    "We save them as pickled objects so we can use them later for training and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_dataset('../data/processed/train_dataset_resized.pckl')\n",
    "test_dataset.save_dataset('../data/processed/test_dataset_resized.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
